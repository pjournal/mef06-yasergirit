[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yaser Girit",
    "section": "",
    "text": "This progress journal covers Yaser Girit’s work during the term at BDA 503 Fall 2022.\nBy clicking the sections left side, you can reach assigments and works."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "1  Assignment 1",
    "section": "",
    "text": "Hi! I am Yaser. After working as a production and planning engineer in different sectors such as automotive, rubber and packaging, currently I am working in a company that monitors and collects global sports data. The project I am responsible for in the company is to list the details obtained by performing analyzes by shooting, game type and game situation, and the collected data is made available to basketball federations, coaches and players. I resigned from my basketball-related job in October 2022. Then, in line with my career goal, I started to work as a planning and data analytics specialist at Avon in November 2022. I was personally interested in data science topics for over two years outside of business life until now. What interests me most about data science is its capability of creating the future and changing traditional approaches inevitably in different areas. My primary goal for the future works is to use the experience I have gained so far and to work on data-driven projects by combining them with the knowledge I will obtain in MEF Big Data Analytics Program."
  },
  {
    "objectID": "assignment1.html#rstudioconf2022-advocating-for-automation",
    "href": "assignment1.html#rstudioconf2022-advocating-for-automation",
    "title": "1  Assignment 1",
    "section": "1.2 rstudio::conf(2022) Advocating for Automation",
    "text": "1.2 rstudio::conf(2022) Advocating for Automation\n\n\n\n\n\n\n\n\n\nIn this session of the conference, Hannah Podzorski talks about how to use R to automate workflows. Automation provides advantages such as reproducibility, simplicity, time saving and less human interaction in processes.\nShe mentions two important R packages she uses. First one was {openxlsx}. It designed to edit, read, edit Excel files in R. Other R package she mentioned is {officer} that allows manipulate Word documents and PowerPoint documents in R. She uses it mostly for exporting ggplot to PowerPoint. She shows that adjustments can be made easily with the {officer} on a graph showing the change of chlorine concentration by time.\nIn a part of her presentation she talks about her team had decided to automate a software named ProUCL that is a statistical software package for analysis of environmental data sets with and without nondetect observations. It developed by the U.S. Environmental Protection Agency (EPA). It shows that they automate the software with mini mouse macros using appropriate functions in R environment. It is generally mentioned that process automations reduce human errors and save time in the presentation. And it seems like how all of this is possible with R.\nSource: https://www.rstudio.com/conference/2022/talks/advocating-for-automation/"
  },
  {
    "objectID": "assignment1.html#r-post",
    "href": "assignment1.html#r-post",
    "title": "1  Assignment 1",
    "section": "1.3 R Post",
    "text": "1.3 R Post\nFollowing section are about 3 R posts relevant to my interests.\n\n1.3.1 Audio Classification in R\nNot long ago, I have worked on a deep learning project in which urban sounds could be classified with the CNN model as part of a bootcamp and I have run the project in python environment."
  },
  {
    "objectID": "assignment1.html#r-posts",
    "href": "assignment1.html#r-posts",
    "title": "1  Assignment 1",
    "section": "1.3 R Posts",
    "text": "1.3 R Posts\nFollowing section are about 3 R posts relevant to my interests.\n\n1.3.1 Audio Classification in R\n\nNot long ago, I have worked on a deep learning project in which urban sounds could be classified with the CNN model as part of a bootcamp and I have run the project in Python environment. You can check it by clicking here. So how to advance such a project in the R environment?\nIn fact, the basic logic in such a project is to visualize the spectral features of the sound, which extracts the frequency and power characteristics and then to classify the extracted features as in image processing techniques.\nYou can see the details at the link below.\nBlog post: https://blogs.rstudio.com/ai/posts/2018-06-06-simple-audio-classification-keras/\n\n\n\n1.3.2 How to Create a Correlation Matrix in R\n\nWe know the correlation matrix as a table of correlation coefficients that shows whether there is a relationship between a set of data. It is a powerful tool to summarize a large dataset and to identify and visualize patterns in the given data.\nThe following 4 different methods of creating a correlation matrix are described in the blog.\n\nThe cor Function (For getting simple matrix of correlation coefficients)\nThe rcorr Function (For getting p-values of correlation coefficients)\nThe corrplot Function (For visualizing correlation matrix)\nThe ggcorrplot Function (For visualizing correlation matrix)\n\n\n#create data frame\ndf <- data.frame(assists=c(4, 5, 5, 6, 7, 8, 8, 10),\n                 rebounds=c(12, 14, 13, 7, 8, 8, 9, 13),\n                 points=c(22, 24, 26, 26, 29, 32, 20, 14))\n\n\n\n#view dataframe\ndf\n\n  assists rebounds points\n1       4       12     22\n2       5       14     24\n3       5       13     26\n4       6        7     26\n5       7        8     29\n6       8        8     32\n7       8        9     20\n8      10       13     14\n\n\n\nLet’s look at the correlation matrix that shows the correlation coefficients between each variable in the data.\n\n\n#create correlation matrix\ncor(df)\n\n            assists   rebounds     points\nassists   1.0000000 -0.2448608 -0.3295730\nrebounds -0.2448608  1.0000000 -0.5220917\npoints   -0.3295730 -0.5220917  1.0000000\n\n\n\nThe corrplot and ggcorrplot packages make it easy to visualize directly.\n\ninstall.packages(\"corrplot\")\n\nInstalling package into '/home/yaser/R/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n\n\n\n#visualize correlation matrix\ncorrplot(cor(df), method = \"color\", type=\"full\", addgrid.col = \"white\",\n        addshade = \"positive\", addCoef.col = \"black\")\n\n\n\n\n\nBlog Post: https://www.statology.org/correlation-matrix-in-r/\n\n\n\n1.3.3 Using dplyr with Databases\n\nIn addition to working with data in local memory stored in dataframes, dplyr also works with data on a remote disk stored in a database.\nThis is especially useful in two scenarios:\n\nYour data is already in the database.\nThere is too much data to fit all in memory at once and you must use an external storage mechanism.\n\ndbplyr package is needed to use databases with dplyr.\n\ninstall.packages(\"dbplyr\")\n\nInstalling package into '/home/yaser/R/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\n\nAlso, DBI backend package is required. The DBI package provides a common interface that allows dplyr to work with different databases using the same code. If it is not already installed, it is automatically installed with the dplyr package.\n\n1.3.3.1 Connecting to Database\nUsing DBI::dbConnect() we can work with databases. The first argument will be the database backend we want to connect to. Five commonly used backends are:\n\nRMySQL connects to MySQL and MariaDB\nRPostgreSQL connects to Postgres and Redshift.\nRSQLite embeds a SQLite database.\nodbc connects to many commercial databases via the open database connectivity protocol.\nbigrquery connects to Google’s BigQuery.\n\nlibrary(dplyr)\ncon <- DBI::dbConnect(RSQLite::SQLite(), path = \":dbname:\")\nThere are simple code blocks for connecting to database in the rest of the blog post. It seems possible to do this with R codes without having to master the SQL syntax.\ncon <- DBI::dbConnect(RMySQL::MySQL(), \n  host = \"database.rstudio.com\",\n  user = \"hadley\",\n  password = rstudioapi::askForPassword(\"Database password\")\n)\n\n\n\n1.3.3.2 Generating Queries\ndplyr prepares the SQL queries itself, but it is still important to know the SQL language at a basic level.\nThere is an example below about how to use SQL statements with it.\nflights_db %>% select(year:day, dep_delay, arr_delay)\n\nflights_db %>% filter(dep_delay > 240)\n\nflights_db %>% \n  group_by(dest) %>%\n  summarise(delay = mean(dep_time))\nBlog Post: https://solutions.rstudio.com/db/r-packages/dplyr/"
  },
  {
    "objectID": "assignment1.html#air-quality",
    "href": "assignment1.html#air-quality",
    "title": "ggplot2 demo",
    "section": "1.4 Air Quality",
    "text": "1.4 Air Quality\nFigure 1.1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\nFigure 1.1: Temperature and ozone level."
  },
  {
    "objectID": "assignment1.html#using-dplyr-with-databases",
    "href": "assignment1.html#using-dplyr-with-databases",
    "title": "1  Assignment 1",
    "section": "1.4 Using dplyr with Databases",
    "text": "1.4 Using dplyr with Databases\n\nIn addition to working with data in local memory stored in dataframes, dplyr also works with data on a remote disk stored in a database.\nThis is especially useful in two scenarios:\n\nYour data is already in the database.\nThere is too much data to fit all in memory at once and you must use an external storage mechanism.\n\ndbplyr package is needed to use databases with dplyr.\n\ninstall.packages(\"dbplyr\")\n\nInstalling package into '/home/yaser/R/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\n\nAlso, DBI backend package is required. The DBI package provides a common interface that allows dplyr to work with different databases using the same code. If it is not already installed, it is automatically installed with the dplyr package.\n\n1.4.1 Connecting to Database\nUsing DBI::dbConnect() we can work with databases. The first argument will be the database backend we want to connect to. Five commonly used backends are:\n\nRMySQL connects to MySQL and MariaDB\nRPostgreSQL connects to Postgres and Redshift.\nRSQLite embeds a SQLite database.\nodbc connects to many commercial databases via the open database connectivity protocol.\nbigrquery connects to Google’s BigQuery.\n\nlibrary(dplyr)\ncon <- DBI::dbConnect(RSQLite::SQLite(), path = \":dbname:\")\nThere are simple code blocks for connecting to database in the rest of the blog post. It seems possible to do this with R codes without having to master the SQL syntax.\ncon <- DBI::dbConnect(RMySQL::MySQL(), \n  host = \"database.rstudio.com\",\n  user = \"hadley\",\n  password = rstudioapi::askForPassword(\"Database password\")\n)\n\n\n\n1.4.2 Generating Queries\ndplyr prepares the SQL queries itself, but it is still important to know the SQL language at a basic level.\nThere is an example below about how to use SQL statements with it.\nflights_db %>% select(year:day, dep_delay, arr_delay)\n\nflights_db %>% filter(dep_delay > 240)\n\nflights_db %>% \n  group_by(dest) %>%\n  summarise(delay = mean(dep_time))\nBlog Post: https://solutions.rstudio.com/db/r-packages/dplyr/"
  },
  {
    "objectID": "inclass1.html",
    "href": "inclass1.html",
    "title": "2  In Class Exercise 1",
    "section": "",
    "text": "2.0.2 Analysis 1 - On Time Arrival Rate for Destinations\n\n\nflights %>%\n  mutate(arr_type = ifelse(arr_delay < 5, \"on time\", \"delayed\")) %>%\n  group_by(dest) %>%\n  summarise(count=n(), ot_arr_rate = sum(arr_type == \"on time\", na.rm=TRUE) / n()*100, \"%\") %>%\n  arrange(desc(ot_arr_rate)) %>%\n  print(n=25)\n\n# A tibble: 105 × 4\n   dest  count ot_arr_rate `\"%\"`\n   <chr> <int>       <dbl> <chr>\n 1 LEX       1       100   %    \n 2 PSP      19        78.9 %    \n 3 SNA     825        74.2 %    \n 4 STT     522        73.6 %    \n 5 MVY     221        72.9 %    \n 6 HNL     707        72.3 %    \n 7 BOS   15508        71.2 %    \n 8 SEA    3923        71.1 %    \n 9 MIA   11728        70.9 %    \n10 SLC    2467        70.5 %    \n11 SBN      10        70   %    \n12 LAS    5997        69.5 %    \n13 LAX   16174        68.1 %    \n14 LGB     668        68.0 %    \n15 ACK     265        67.9 %    \n16 DFW    8738        67.7 %    \n17 SJC     329        67.5 %    \n18 SRQ    1211        67.5 %    \n19 SJU    5819        67.2 %    \n20 DTW    9384        67.1 %    \n21 SFO   13331        66.9 %    \n22 MTJ      15        66.7 %    \n23 MYR      59        66.1 %    \n24 MCO   14082        65.6 %    \n25 PHX    4656        65.5 %    \n# … with 80 more rows\n\n\n\n\n\n2.0.3 Analysis 2 - Departure Delay by Months\n\n\nflights %>%\n  group_by(month) %>%\n  summarize(count = n(),\n    avg_dep_delay = mean(dep_delay, na.rm = TRUE),\n    max_dep_delay = max(dep_delay, na.rm = TRUE),\n    min_dep_delay = min(dep_delay, na.rm = TRUE),    \n  )\n\n# A tibble: 12 × 5\n   month count avg_dep_delay max_dep_delay min_dep_delay\n   <int> <int>         <dbl>         <dbl>         <dbl>\n 1     1 27004         10.0           1301           -30\n 2     2 24951         10.8            853           -33\n 3     3 28834         13.2            911           -25\n 4     4 28330         13.9            960           -21\n 5     5 28796         13.0            878           -24\n 6     6 28243         20.8           1137           -21\n 7     7 29425         21.7           1005           -22\n 8     8 29327         12.6            520           -26\n 9     9 27574          6.72          1014           -24\n10    10 28889          6.24           702           -25\n11    11 27268          5.44           798           -32\n12    12 28135         16.6            896           -43"
  },
  {
    "objectID": "shinyapp.html",
    "href": "shinyapp.html",
    "title": "3  Shiny App",
    "section": "",
    "text": "This application, which works with FluidRows, enables users filtering of foreign student nationality data of universities taken from Higher Education Council’s (YÖK) statistics portal. The desired values can be selected separately from the University Name, University Type, City Name and Nationality columns, or filtering can be done by selecting the entire column. Click on the fluid rows and simply select the values you want.\n\nApplication Link: https://yasergirit.shinyapps.io/ShinyApp/"
  },
  {
    "objectID": "operationsResearch.html",
    "href": "operationsResearch.html",
    "title": "4  Operations Research",
    "section": "",
    "text": "The business case described in this case study is that of Emesa, a Dutch company that operates an online flower delivery service. Emesa faced a problem with its delivery logistics, as the company was struggling to effectively plan and schedule its deliveries in a way that would meet customer demand while also maximizing efficiency and minimizing costs.\n\n\n\n Image Source\n\n\n\n\n\n\nTo solve this problem, Emesa turned to Gurobi, a mathematical optimization software company, for help. Gurobi’s optimization algorithms were used to build a customized delivery planning and scheduling model for Emesa, which took into account various constraints and objectives such as delivery capacity, vehicle availability, and delivery time windows.\nThe model was able to provide Emesa with optimal delivery plans that were both cost-effective and feasible to execute. This allowed Emesa to better meet customer demand, reduce costs, and increase efficiency.\n\n\n\n\n\nOne key benefit of using the Gurobi model was that it allowed Emesa to make more informed and strategic decisions about its delivery operations. By using data and optimization algorithms, Emesa was able to better understand its delivery capabilities and identify opportunities to optimize its operations. This helped the company to improve its planning and scheduling processes, and ultimately led to cost savings and increased customer satisfaction.\nAnother benefit of the Gurobi model was that it allowed Emesa to be more flexible and adaptable in its operations. With the ability to quickly generate and evaluate different delivery plans, Emesa was able to respond to changes in demand or other variables in real-time, ensuring that it was always operating at peak efficiency.\nThe implementation of the Gurobi delivery planning and scheduling model was a success for Emesa. It helped the company to streamline its operations, reduce costs, and improve customer satisfaction, leading to a positive impact on the company’s bottom line."
  }
]