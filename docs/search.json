[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yaser Girit",
    "section": "",
    "text": "This progress journal covers Yaser Girit’s work during the term at BDA 503 Fall 2022.\nBy clicling the sections left side, you can reach assigments and works."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "1  Assignment 1",
    "section": "",
    "text": "Hi! I am Yaser. After working as a production and planning engineer in different sectors such as automotive, rubber and packaging, currently I am working in a company that monitors and collects global sports data. The project I am responsible for in the company is to list the details obtained by performing analyzes by shooting, game type and game situation, and the collected data is made available to basketball federations, coaches and players. I have been personally interested in data science topics for over two years outside of business life. What interests me most about data science is its capability of creating the future and changing traditional approaches inevitably in different areas. My primary goal for the future works is to use the experience I have gained in production processes as domain knowledge and to work on data-driven projects in the production sector by combining them with the knowledge I will obtain in MEF Big Data Analytics Program."
  },
  {
    "objectID": "assignment1.html#rstudioconf2022-advocating-for-automation",
    "href": "assignment1.html#rstudioconf2022-advocating-for-automation",
    "title": "1  Assignment 1",
    "section": "1.2 rstudio::conf(2022) Advocating for Automation",
    "text": "1.2 rstudio::conf(2022) Advocating for Automation\n\n\n\n\n\n\n\n\n\nIn this session of the conference, Hannah Podzorski talks about how to use R to automate workflows. Automation provides advantages such as reproducibility, simplicity, time saving and less human interaction in processes.\nShe mentions two important R packages she uses. First one was {openxlsx}. It designed to edit, read, edit Excel files in R. Other R package she mentioned is {officer} that allows manipulate Word documents and PowerPoint documents in R. She uses it mostly for exporting ggplot to PowerPoint. She shows that adjustments can be made easily with the {officer} on a graph showing the change of chlorine concentration by time.\nIn a part of her presentation she talks about her team had decided to automate a software named ProUCL that is a statistical software package for analysis of environmental data sets with and without nondetect observations. It developed by the U.S. Environmental Protection Agency (EPA). It shows that they automate the software with mini mouse macros using appropriate functions in R environment. It is generally mentioned that process automations reduce human errors and save time in the presentation. And it seems like how all of this is possible with R.\nSource: https://www.rstudio.com/conference/2022/talks/advocating-for-automation/"
  },
  {
    "objectID": "assignment1.html#r-post",
    "href": "assignment1.html#r-post",
    "title": "1  Assignment 1",
    "section": "1.3 R Post",
    "text": "1.3 R Post\nFollowing section are about 3 R posts relevant to my interests.\n\n1.3.1 Audio Classification in R\nNot long ago, I have worked on a deep learning project in which urban sounds could be classified with the CNN model as part of a bootcamp and I have run the project in python environment."
  },
  {
    "objectID": "assignment1.html#r-posts",
    "href": "assignment1.html#r-posts",
    "title": "1  Assignment 1",
    "section": "1.3 R Posts",
    "text": "1.3 R Posts\nFollowing section are about 3 R posts relevant to my interests.\n\n1.3.1 Audio Classification in R\n\nNot long ago, I have worked on a deep learning project in which urban sounds could be classified with the CNN model as part of a bootcamp and I have run the project in Python environment. You can check it by clicking here. So how to advance such a project in the R environment?\nIn fact, the basic logic in such a project is to visualize the spectral features of the sound, which extracts the frequency and power characteristics and then to classify the extracted features as in image processing techniques.\nYou can see the details at the link below.\nBlog post: https://blogs.rstudio.com/ai/posts/2018-06-06-simple-audio-classification-keras/\n\n\n\n1.3.2 How to Create a Correlation Matrix in R\n\nWe know the correlation matrix as a table of correlation coefficients that shows whether there is a relationship between a set of data. It is a powerful tool to summarize a large dataset and to identify and visualize patterns in the given data.\nThe following 4 different methods of creating a correlation matrix are described in the blog.\n\nThe cor Function (For getting simple matrix of correlation coefficients)\nThe rcorr Function (For getting p-values of correlation coefficients)\nThe corrplot Function (For visualizing correlation matrix)\nThe ggcorrplot Function (For visualizing correlation matrix)\n\n\n#create data frame\ndf <- data.frame(assists=c(4, 5, 5, 6, 7, 8, 8, 10),\n                 rebounds=c(12, 14, 13, 7, 8, 8, 9, 13),\n                 points=c(22, 24, 26, 26, 29, 32, 20, 14))\n\n\n\n#view dataframe\ndf\n\n  assists rebounds points\n1       4       12     22\n2       5       14     24\n3       5       13     26\n4       6        7     26\n5       7        8     29\n6       8        8     32\n7       8        9     20\n8      10       13     14\n\n\n\nLet’s look at the correlation matrix that shows the correlation coefficients between each variable in the data.\n\n\n#create correlation matrix\ncor(df)\n\n            assists   rebounds     points\nassists   1.0000000 -0.2448608 -0.3295730\nrebounds -0.2448608  1.0000000 -0.5220917\npoints   -0.3295730 -0.5220917  1.0000000\n\n\n\nThe corrplot and ggcorrplot packages make it easy to visualize directly.\n\ninstall.packages(\"corrplot\")\n\nInstalling package into '/home/yaser/R/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n\n\n\n#visualize correlation matrix\ncorrplot(cor(df), method = \"color\", type=\"full\", addgrid.col = \"white\",\n        addshade = \"positive\", addCoef.col = \"black\")\n\n\n\n\n\nBlog Post: https://www.statology.org/correlation-matrix-in-r/\n\n\n\n1.3.3 Using dplyr with Databases\n\nIn addition to working with data in local memory stored in dataframes, dplyr also works with data on a remote disk stored in a database.\nThis is especially useful in two scenarios:\n\nYour data is already in the database.\nThere is too much data to fit all in memory at once and you must use an external storage mechanism.\n\ndbplyr package is needed to use databases with dplyr.\n\ninstall.packages(\"dbplyr\")\n\nInstalling package into '/home/yaser/R/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\n\nAlso, DBI backend package is required. The DBI package provides a common interface that allows dplyr to work with different databases using the same code. If it is not already installed, it is automatically installed with the dplyr package.\n\n1.3.3.1 Connecting to Database\nUsing DBI::dbConnect() we can work with databases. The first argument will be the database backend we want to connect to. Five commonly used backends are:\n\nRMySQL connects to MySQL and MariaDB\nRPostgreSQL connects to Postgres and Redshift.\nRSQLite embeds a SQLite database.\nodbc connects to many commercial databases via the open database connectivity protocol.\nbigrquery connects to Google’s BigQuery.\n\nlibrary(dplyr)\ncon <- DBI::dbConnect(RSQLite::SQLite(), path = \":dbname:\")\nThere are simple code blocks for connecting to database in the rest of the blog post. It seems possible to do this with R codes without having to master the SQL syntax.\ncon <- DBI::dbConnect(RMySQL::MySQL(), \n  host = \"database.rstudio.com\",\n  user = \"hadley\",\n  password = rstudioapi::askForPassword(\"Database password\")\n)\n\n\n\n1.3.3.2 Generating Queries\ndplyr prepares the SQL queries itself, but it is still important to know the SQL language at a basic level.\nThere is an example below about how to use SQL statements with it.\nflights_db %>% select(year:day, dep_delay, arr_delay)\n\nflights_db %>% filter(dep_delay > 240)\n\nflights_db %>% \n  group_by(dest) %>%\n  summarise(delay = mean(dep_time))\nBlog Post: https://solutions.rstudio.com/db/r-packages/dplyr/"
  },
  {
    "objectID": "assignment1.html#air-quality",
    "href": "assignment1.html#air-quality",
    "title": "ggplot2 demo",
    "section": "1.4 Air Quality",
    "text": "1.4 Air Quality\nFigure 1.1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\nFigure 1.1: Temperature and ozone level."
  },
  {
    "objectID": "assignment1.html#using-dplyr-with-databases",
    "href": "assignment1.html#using-dplyr-with-databases",
    "title": "1  Assignment 1",
    "section": "1.4 Using dplyr with Databases",
    "text": "1.4 Using dplyr with Databases\n\nIn addition to working with data in local memory stored in dataframes, dplyr also works with data on a remote disk stored in a database.\nThis is especially useful in two scenarios:\n\nYour data is already in the database.\nThere is too much data to fit all in memory at once and you must use an external storage mechanism.\n\ndbplyr package is needed to use databases with dplyr.\n\ninstall.packages(\"dbplyr\")\n\nInstalling package into '/home/yaser/R/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\n\nAlso, DBI backend package is required. The DBI package provides a common interface that allows dplyr to work with different databases using the same code. If it is not already installed, it is automatically installed with the dplyr package.\n\n1.4.1 Connecting to Database\nUsing DBI::dbConnect() we can work with databases. The first argument will be the database backend we want to connect to. Five commonly used backends are:\n\nRMySQL connects to MySQL and MariaDB\nRPostgreSQL connects to Postgres and Redshift.\nRSQLite embeds a SQLite database.\nodbc connects to many commercial databases via the open database connectivity protocol.\nbigrquery connects to Google’s BigQuery.\n\nlibrary(dplyr)\ncon <- DBI::dbConnect(RSQLite::SQLite(), path = \":dbname:\")\nThere are simple code blocks for connecting to database in the rest of the blog post. It seems possible to do this with R codes without having to master the SQL syntax.\ncon <- DBI::dbConnect(RMySQL::MySQL(), \n  host = \"database.rstudio.com\",\n  user = \"hadley\",\n  password = rstudioapi::askForPassword(\"Database password\")\n)\n\n\n\n1.4.2 Generating Queries\ndplyr prepares the SQL queries itself, but it is still important to know the SQL language at a basic level.\nThere is an example below about how to use SQL statements with it.\nflights_db %>% select(year:day, dep_delay, arr_delay)\n\nflights_db %>% filter(dep_delay > 240)\n\nflights_db %>% \n  group_by(dest) %>%\n  summarise(delay = mean(dep_time))\nBlog Post: https://solutions.rstudio.com/db/r-packages/dplyr/"
  }
]